{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Objetivo:\n",
        " * Implementar uma Rede Neural Recorrente (RNN) simples em Python pra prever a próxima palavra em uma sequência de texto, utilizando biblioteca TensorFlow/Keras.\n",
        " * Implementar uma Rede Long Short-Term Memory (LSTM) em Python para classificar o sentimento de frases como \"positivo\" ou \"negativo\", utilizando a biblioteca TensorFlow/Keras."
      ],
      "metadata": {
        "id": "q_Yb1ErXwvev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Etapas de Desenvolvimento (Fluxo do Programa):"
      ],
      "metadata": {
        "id": "7LFovmt5vupw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Implementação: RNN Simples para Previsão da Próxima Palavra\n",
        "\n",
        "* Etapa 1: Preparação dos dados: Coletar e pré-processar um corpus de texto para treinamento.\n",
        "* Etapa 2: Construção do modelo RNN: Definir a arquitetura da RNN simples.\n",
        "* Etapa 3: Treinamento do modelo: Treinar a RNN usando o corpus preparado.\n",
        "* Etapa 4: Avaliação do modelo: Avaliar o desempenho da RNN na previsão da próxima palavra.\n",
        "* Etapa 5: Teste do modelo: Testar o modelo com novas frases."
      ],
      "metadata": {
        "id": "NmLqy9Y_vx8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Implementação 2: LSTMs para Classificação de Sentimentos\n",
        "\n",
        "* Etapa 1: Preparação dos dados: Coletar e pré-processar um conjunto de frases rotulados (positivo/negativo)\n",
        "* Etapa 2: Construção do modelo LSTM: Definir a arquitetura da LSTM.\n",
        "* Etapa 3: Treinamento do modelo: Treinar a LSTM usando os dados rotulados\n",
        "* Etapa 4: Avaliação do modelo: Avaliar desempenho da LSTM na classificação de sentimentos\n",
        "* Etapa 5: Teste do modelo: Testar o modelo com novas fases"
      ],
      "metadata": {
        "id": "4mKmiKucv0R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementação 1: Modelo de Rede Neural de Recorrência"
      ],
      "metadata": {
        "id": "4VIY177uv4TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 1: Configuração de Ambiente no Google Colab"
      ],
      "metadata": {
        "id": "EBqoh3Qyv6cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa a biblioteca NumPy, usada para operações numéricas e manipulação de arrays\n",
        "import numpy as np\n",
        "\n",
        "# Importa o TensorFlow, um framework de aprendizado de máquina amplamente utilizado\n",
        "import tensorflow as tf\n",
        "\n",
        "# Importa a classe Sequential, que permite criar modelos de rede neural camada por camada\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Importa as camadas utilizadas: Embedding, SimpleRNN e Dense\n",
        "# Embedding transforma palavras em vetores densos\n",
        "# SimpleRNN é uma rede neural recorrente simples\n",
        "# Dense é uma camada totalmente conectada\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Importa o Tokenizer, ferramenta para transformar texto em sequências de números inteiros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Importa a função pad_sequences, que ajusta o comprimento das sequências de entrada (completa ou corta)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Exibe mensagem indicando que todas as bibliotecas foram importadas com sucesso\n",
        "print(\"Bibliotecas importadas com sucesso\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kKxqme6wp-9",
        "outputId": "1e89aefe-64f8-4941-ac84-38fb1634b53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicação:\n",
        "* numpy: Para operações numéricas\n",
        "* tensorflow.keras: A API de alto nível para construir e treinar modelos de deep learning\n",
        "* Embedding: Camada que transforma palavras (indices numéricos) em vetores densos\n",
        "* SimpleRNN: A camada de Rede Neural Recorrente mais básica\n",
        "* Dense: Camada neural comum (fully connected layer)\n",
        "* Tokenizer: Para converter texto em sequências de números\n",
        "* pad_sequences: Para garantir que todas as sequências de entrada tenham o mesmo comprimento"
      ],
      "metadata": {
        "id": "E85KGRZkxrDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passo 2: Preparação do Conjunto de Dados"
      ],
      "metadata": {
        "id": "aRgHbyYxxtjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define um pequeno conjunto de textos que será usado como base para treinar a rede neural\n",
        "# Cada item da lista é uma frase curta em português\n",
        "textos_treinamento = [\n",
        "    \"eu gosto de programar em python\",\n",
        "    \"python é uma linguagem poderosa\",\n",
        "    \"programar é divertido com python\",\n",
        "    \"aprenda python e seja feliz\",\n",
        "    \"gosto de aprender coisas novas\"\n",
        "]\n",
        "\n",
        "# Exibe no console os textos definidos para o treinamento\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sXL5TULx5qR",
        "outputId": "2da020e0-cc69-496c-987c-044665baedab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programar é divertido com python', 'aprenda python e seja feliz', 'gosto de aprender coisas novas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o Tokenizer, que será responsável por converter palavras em números inteiros\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Gera o vocabulário com base nos textos de treinamento fornecidos\n",
        "# Cada palavra única será associada a um índice numérico\n",
        "tokenizer.fit_on_texts(textos_treinamento)\n",
        "\n",
        "# Converte cada frase em uma sequência de números inteiros de acordo com o vocabulário criado\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "\n",
        "# Exibe no console o vocabulário mapeado (palavra: índice)\n",
        "print(f\"\\nVocabulário (palavra:índice): {tokenizer.word_index}\")\n",
        "\n",
        "# Exibe as frases convertidas em sequências numéricas\n",
        "print(f\"Sequencias numéricas dos textos: {sequencias}\")\n",
        "\n",
        "# Calcula o tamanho do vocabulário total\n",
        "# Soma 1 ao total de palavras para considerar o índice 0 usado em padding\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Exibe a quantidade total de palavras (dimensão do vocabulário)\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBUK7ODzyLJn",
        "outputId": "4f8a9b91-737c-41b7-ae9a-ac699e780b2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulário (palavra:índice): {'python': 1, 'gosto': 2, 'de': 3, 'programar': 4, 'é': 5, 'eu': 6, 'em': 7, 'uma': 8, 'linguagem': 9, 'poderosa': 10, 'divertido': 11, 'com': 12, 'aprenda': 13, 'e': 14, 'seja': 15, 'feliz': 16, 'aprender': 17, 'coisas': 18, 'novas': 19}\n",
            "Sequencias numéricas dos textos: [[6, 2, 3, 4, 7, 1], [1, 5, 8, 9, 10], [4, 5, 11, 12, 1], [13, 1, 14, 15, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparação das entradas (X) e saídas (y) para o treinamento do modelo de previsão da próxima palavra\n",
        "\n",
        "# Define o comprimento máximo entre todas as sequências geradas anteriormente\n",
        "# Isso será usado para aplicar o padding de forma consistente\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"\\nComprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "# Inicializa listas para armazenar os pares de treino: entradas (X) e saídas (y)\n",
        "entradas_X = []\n",
        "saidas_y = []\n",
        "\n",
        "# Para cada sequência completa, gera todos os pares de entrada e saída possíveis\n",
        "# Exemplo: para [1, 2, 3, 4], gera pares ([1] → 2), ([1, 2] → 3), ([1, 2, 3] → 4)\n",
        "for seq in sequencias:\n",
        "  for i in range(1, len(seq)):\n",
        "    entradas_X.append(seq[:i])   # Parte inicial da sequência (entrada)\n",
        "    saidas_y.append(seq[i])      # Próxima palavra da sequência (saída)\n",
        "\n",
        "# Exibe exemplos das entradas e saídas geradas para verificação\n",
        "print(f\"Exemplo de entradas_x (parcial): {entradas_X[0:5]}\")\n",
        "print(f\"Exemplo de entradas_y (parcial): {saidas_y[0:5]}\")\n",
        "\n",
        "# Aplica padding às entradas para que todas tenham o mesmo comprimento\n",
        "# O padding é feito à esquerda ('pre') com zeros\n",
        "# O comprimento usado é max_comprimento - 1, pois a entrada deve ter uma palavra a menos que a sequência original\n",
        "entradas_X_padded = pad_sequences(entradas_X, maxlen=max_comprimento - 1, padding='pre')\n",
        "\n",
        "# Converte as saídas para o formato one-hot encoding\n",
        "# Isso transforma cada índice (palavra de saída) em um vetor binário com 1 na posição correspondente\n",
        "# Esse formato é compatível com a saída softmax da rede neural\n",
        "saidas_y_one_hot = tf.keras.utils.to_categorical(saidas_y, num_classes=total_palavras)\n",
        "\n",
        "# Exibe as versões processadas das entradas e saídas para verificação\n",
        "print(f\"\\nExemplo de entradas_x_padded (após padding e truncagem): \\n{entradas_X_padded[0:5]}\")\n",
        "print(f\"\\nExemplo de saidas_y_one_hot (após one-hot encoding): \\n{saidas_y_one_hot[0:5]}\")\n",
        "\n",
        "# Mostra as dimensões finais dos arrays de entrada e saída usados no treinamento\n",
        "print(f\"\\nFormato final das entradas (X): {entradas_X_padded.shape}\")\n",
        "print(f\"\\nExemplo final das saídas (y): {saidas_y_one_hot.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Rdtbe2ykeU",
        "outputId": "7311a4b2-901a-4888-cb39-c538c744e467"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas_x (parcial): [[6], [6, 2], [6, 2, 3], [6, 2, 3, 4], [6, 2, 3, 4, 7]]\n",
            "Exemplo de entradas_y (parcial): [2, 3, 4, 7, 1]\n",
            "\n",
            "Exemplo de entradas_x_padded (após padding e truncagem): \n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 6 2]\n",
            " [0 0 6 2 3]\n",
            " [0 6 2 3 4]\n",
            " [6 2 3 4 7]]\n",
            "\n",
            "Exemplo de saidas_y_one_hot (após one-hot encoding): \n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "Formato final das entradas (X): (21, 5)\n",
            "\n",
            "Exemplo final das saídas (y): (21, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicação:**\n",
        "\n",
        "- numpy: Biblioteca para operações matemáticas e arrays multidimensionais.\n",
        "- tensorflow.keras: API simplificada do TensorFlow para criar modelos de deep learning.\n",
        "- Embedding: Converte índices de palavras em vetores densos com significado semântico.\n",
        "- SimpleRNN: Rede neural recorrente básica que processa sequências mantendo memória temporal.\n",
        "- Dense: Camada neural totalmente conectada (cada neurônio conecta-se a todos da camada anterior).\n",
        "- Tokenizer: Converte texto em sequências numéricas criando um vocabulário.\n",
        "- pad_sequences: Padroniza o tamanho das sequências adicionando zeros ou truncando."
      ],
      "metadata": {
        "id": "D8O8udcqyp21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3: Construção do Modelo RNN"
      ],
      "metadata": {
        "id": "cGOXZ9Y3yv2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição e construção da arquitetura do modelo de rede neural recorrente (RNN)\n",
        "\n",
        "# Cria uma instância do modelo sequencial (camadas empilhadas linearmente)\n",
        "modelo_rnn = Sequential()\n",
        "\n",
        "# Adiciona a camada de Embedding ao modelo\n",
        "# Esta camada transforma cada índice de palavra (inteiro) em um vetor denso (embedding)\n",
        "# total_palavras: número total de palavras únicas no vocabulário (dimensão do vocabulário)\n",
        "# 10: dimensão dos vetores de embedding (representação vetorial de cada palavra)\n",
        "# input_length: comprimento fixo das sequências de entrada (após padding)\n",
        "modelo_rnn.add(Embedding(input_dim=total_palavras, output_dim=10, input_length=entradas_X_padded.shape[1]))\n",
        "\n",
        "# Adiciona a camada SimpleRNN ao modelo\n",
        "# Esta é uma camada de rede neural recorrente simples, que processa sequências passo a passo\n",
        "# 32: número de neurônios (unidades ocultas) na camada, que representam o estado interno\n",
        "modelo_rnn.add(SimpleRNN(units=32))\n",
        "\n",
        "# Adiciona a camada densa de saída ao modelo\n",
        "# total_palavras: número de neurônios na saída, um para cada possível palavra a ser prevista\n",
        "# activation='softmax': transforma a saída em uma distribuição de probabilidade entre as palavras\n",
        "modelo_rnn.add(Dense(units=total_palavras, activation='softmax'))\n",
        "\n",
        "# Compila o modelo\n",
        "# optimizer='adam': otimizador eficiente que ajusta os pesos da rede\n",
        "# loss='categorical_crossentropy': função de perda apropriada para classificação com múltiplas classes (one-hot)\n",
        "# metrics=['accuracy']: métrica usada para avaliar o desempenho durante o treinamento\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibe um resumo da estrutura do modelo\n",
        "# Mostra as camadas, formatos de entrada/saída e número de parâmetros treináveis\n",
        "modelo_rnn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "tlx3etr5z0Hp",
        "outputId": "74aac300-066a-4e63-94f5-170a42475c4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arquitetura do Modelo:**\n",
        "\n",
        "- Embedding Layer: Componente fundamental em PLN que converte índices numéricos de palavras em vetores densos, capturando relações semânticas onde palavras similares ficam próximas no espaço vetorial.\n",
        "- SimpleRNN Layer: Camada recorrente que processa sequências de embeddings sequencialmente, mantendo um estado oculto de dimensão 32 que funciona como \"memória\" temporal da rede.\n",
        "- Dense (Output) Layer: Camada final que transforma o estado oculto da RNN em probabilidades para cada palavra do vocabulário usando softmax, garantindo que as probabilidades somem 1."
      ],
      "metadata": {
        "id": "SGtsOBfSz4Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Passo 4: Treinamento do Modelo"
      ],
      "metadata": {
        "id": "K7Wd-1vOz9CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Início do treinamento do modelo de rede neural recorrente (RNN)\n",
        "\n",
        "# Exibe uma mensagem informando que o treinamento está começando\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "\n",
        "# Treina o modelo utilizando os dados de entrada (X) e saída (y)\n",
        "# entradas_X_padded: sequências de entrada já padronizadas (com padding)\n",
        "# saidas_y_one_hot: saídas convertidas para formato one-hot encoding\n",
        "# epochs=100: número de épocas, ou seja, quantas vezes o modelo verá todo o conjunto de dados\n",
        "# verbose=1: modo detalhado, exibe o progresso do treinamento em cada época\n",
        "modelo_rnn.fit(entradas_X_padded, saidas_y_one_hot, epochs=100, verbose=1)\n",
        "\n",
        "# Exibe mensagem informando que o treinamento foi finalizado\n",
        "print(\"Treinamento concluído!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31QeBIjc0HFf",
        "outputId": "e2ffd6a3-d954-48c4-82f0-b08a44653d46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0476 - loss: 3.0004\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.0476 - loss: 2.9911\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.0476 - loss: 2.9819\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.0952 - loss: 2.9727\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.0952 - loss: 2.9633\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.0952 - loss: 2.9539\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.1429 - loss: 2.9443\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.1429 - loss: 2.9344\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.1905 - loss: 2.9244\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.1905 - loss: 2.9140\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.1905 - loss: 2.9033\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.1905 - loss: 2.8922\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.1905 - loss: 2.8808\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.1905 - loss: 2.8689\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.1905 - loss: 2.8567\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.1905 - loss: 2.8440\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.1905 - loss: 2.8309\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1905 - loss: 2.8173\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1905 - loss: 2.8034\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1905 - loss: 2.7891\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1905 - loss: 2.7744\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1905 - loss: 2.7594\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1905 - loss: 2.7441\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1905 - loss: 2.7286\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.1905 - loss: 2.7129\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1905 - loss: 2.6970\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.1905 - loss: 2.6809\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1905 - loss: 2.6647\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1905 - loss: 2.6484\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1905 - loss: 2.6319\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1905 - loss: 2.6152\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1905 - loss: 2.5982\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1905 - loss: 2.5810\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1905 - loss: 2.5633\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1905 - loss: 2.5453\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1905 - loss: 2.5268\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2381 - loss: 2.5078\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2381 - loss: 2.4882\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2381 - loss: 2.4680\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2381 - loss: 2.4472\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2381 - loss: 2.4258\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2381 - loss: 2.4038\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2857 - loss: 2.3812\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 2.3581\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3333 - loss: 2.3344\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3333 - loss: 2.3103\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3333 - loss: 2.2857\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.3333 - loss: 2.2607\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.3333 - loss: 2.2354\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3333 - loss: 2.2097\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3333 - loss: 2.1838\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3810 - loss: 2.1577\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4286 - loss: 2.1313\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4286 - loss: 2.1049\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4286 - loss: 2.0783\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4286 - loss: 2.0516\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4286 - loss: 2.0249\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4762 - loss: 1.9982\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4762 - loss: 1.9715\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5238 - loss: 1.9449\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5714 - loss: 1.9184\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6190 - loss: 1.8920\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6190 - loss: 1.8657\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6190 - loss: 1.8397\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6190 - loss: 1.8138\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6190 - loss: 1.7882\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 1.7628\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 1.7376\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 1.7127\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7143 - loss: 1.6881\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7143 - loss: 1.6638\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7143 - loss: 1.6397\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7143 - loss: 1.6160\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7143 - loss: 1.5924\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7619 - loss: 1.5692\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7619 - loss: 1.5462\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7619 - loss: 1.5235\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7619 - loss: 1.5010\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8095 - loss: 1.4788\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8095 - loss: 1.4568\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8095 - loss: 1.4350\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8095 - loss: 1.4134\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 1.3921\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8095 - loss: 1.3709\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8095 - loss: 1.3500\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8095 - loss: 1.3293\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8095 - loss: 1.3087\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8095 - loss: 1.2884\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8095 - loss: 1.2683\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8095 - loss: 1.2484\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8095 - loss: 1.2287\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8095 - loss: 1.2092\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8095 - loss: 1.1899\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8095 - loss: 1.1709\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8571 - loss: 1.1521\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8571 - loss: 1.1335\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8571 - loss: 1.1152\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8571 - loss: 1.0971\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8571 - loss: 1.0793\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8571 - loss: 1.0617\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 5: Usar o modelo para Previsão"
      ],
      "metadata": {
        "id": "aajh8NsD0NKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Função de Previsão:\n",
        "# Define uma função para prever a próxima palavra com base em um texto de entrada\n",
        "\n",
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "    # Converte o texto base em uma sequência numérica usando o tokenizer\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([texto_base])[0]\n",
        "\n",
        "    # Padroniza o comprimento da sequência com padding à esquerda (pré)\n",
        "    # Isso garante que a entrada tenha o mesmo comprimento usado durante o treinamento\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "    # Realiza a previsão com o modelo, obtendo as probabilidades para cada palavra do vocabulário\n",
        "    previsao_probabilidades = modelo.predict(sequencia_padded, verbose=0)[0]\n",
        "\n",
        "    # Identifica o índice da palavra com maior probabilidade\n",
        "    indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "    # Converte o índice previsto de volta para a palavra correspondente no vocabulário\n",
        "    for palavra, indice in tokenizer.word_index.items():\n",
        "        if indice == indice_palavra_prevista:\n",
        "            return palavra\n",
        "\n",
        "    # Caso nenhum índice seja encontrado (caso muito raro), retorna None\n",
        "    return None\n",
        "\n",
        "# Define o comprimento das sequências de entrada esperadas pelo modelo\n",
        "# Utiliza o mesmo comprimento usado na etapa de padding (entradas_X_padded.shape[1])\n",
        "comprimento_entrada_modelo = entradas_X_padded.shape[1]\n",
        "\n",
        "# Inicia os testes da função com diferentes frases\n",
        "print(\"\\n--- Testando com Modelo RNN ---\")\n",
        "\n",
        "# Teste 1: entrada parcial \"eu gosto de\"\n",
        "texto_teste_1 = \"eu gosto de\"\n",
        "proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_1)\n",
        "print(f\"Texto: '{texto_teste_1}' -> Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "# Teste 2: entrada parcial \"python é uma\"\n",
        "texto_teste_2 = \"python é uma\"\n",
        "proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_2)\n",
        "print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "# Teste 3: entrada parcial \"programar é divertido\"\n",
        "texto_teste_3 = \"programar é divertido\"\n",
        "proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_3)\n",
        "print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "# Teste 4: entrada parcial \"aprenda python e\"\n",
        "texto_teste_4 = \"aprenda python e\"\n",
        "proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_4)\n",
        "print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "# Teste 5: entrada com palavras fora do vocabulário\n",
        "# Como \"sol\" e \"brilha\" não aparecem nos textos de treinamento, devem ser ignoradas pelo tokenizer\n",
        "texto_teste_5 = \"o sol brilha no\"\n",
        "proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, comprimento_entrada_modelo, texto_teste_5)\n",
        "print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eeugeNf0aOw",
        "outputId": "ecb2b4a7-0aa0-4bb3-f69e-bedef704f568"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando com Modelo RNN ---\n",
            "Texto: 'eu gosto de' -> Próxima palavra prevista: 'programar'\n",
            "Texto: 'python é uma' -> Próxima palavra prevista: 'linguagem'\n",
            "Texto: 'programar é divertido' -> Próxima palavra prevista: 'com'\n",
            "Texto: 'aprenda python e' -> Próxima palavra prevista: 'seja'\n",
            "Texto: 'o sol brilha no' -> Próxima palavra prevista: 'é'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação 2: Modelo de Rede Neural Rede Long Short-Term Memory"
      ],
      "metadata": {
        "id": "6xy1Ffhj0eAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 1: Configuração do Ambiente e Importação de Bibliotecas"
      ],
      "metadata": {
        "id": "s9ElMol00f1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas essenciais para o projeto de classificação de texto com LSTM\n",
        "\n",
        "# Biblioteca para manipulação de arrays e operações numéricas\n",
        "import numpy as np\n",
        "\n",
        "# Biblioteca TensorFlow, usada para construir e treinar redes neurais\n",
        "import tensorflow as tf\n",
        "\n",
        "# Módulos de alto nível da Keras, integrados ao TensorFlow, para construção de modelos sequenciais\n",
        "from tensorflow.keras.models import Sequential               # Modelo sequencial (camadas empilhadas)\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense   # Camadas: Embedding (vetorização), LSTM (memória), Dense (saída)\n",
        "\n",
        "# Ferramentas para pré-processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer    # Transforma texto em sequências numéricas\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Garante que todas as sequências tenham mesmo comprimento\n",
        "\n",
        "# Função para dividir os dados em conjuntos de treino e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Métricas para avaliação do modelo: relatório de classificação e matriz de confusão\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Bibliotecas para visualização de dados e gráficos\n",
        "import matplotlib.pyplot as plt  # Geração de gráficos e visualizações\n",
        "import seaborn as sns            # Visualização estatística avançada baseada em Matplotlib\n",
        "\n",
        "# Mensagem de confirmação ao finalizar as importações\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEH8YeVi0u75",
        "outputId": "bdbd7081-0c9f-4816-91f2-288a39687bb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Novos Componentes Introduzidos:**\n",
        "\n",
        "- LSTM: Versão avançada de RNN que resolve o problema do gradiente que desaparece, usando gates para controlar o fluxo de informação e mantendo memória de longo prazo mais eficazmente.\n",
        "- train_test_split (sklearn): Função que divide automaticamente o dataset em conjuntos de treino e teste, garantindo separação adequada dos dados para validação do modelo.\n",
        "- classification_report, confusion_matrix (sklearn): Ferramentas de avaliação que fornecem métricas detalhadas de performance (precisão, recall, F1-score) e matriz de confusão para análise de erros de classificação.\n",
        "- matplotlib.pyplot, seaborn: Bibliotecas de visualização para criar gráficos e plots que ajudam na interpretação dos resultados e performance do modelo."
      ],
      "metadata": {
        "id": "9BCeP5TX0zXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 2: Preparação do Conjunto de Dados e Análise de Sentimentos"
      ],
      "metadata": {
        "id": "TfMhcV2X03FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o conjunto de dados para análise de sentimentos\n",
        "# Cada item é uma tupla com uma frase (texto) e seu respectivo rótulo de sentimento (positivo ou negativo)\n",
        "\n",
        "dados_sentimento = [\n",
        "    (\"este filme é ótimo e divertido\", \"positivo\"),\n",
        "    (\"eu adorei o livro, muito bom\", \"positivo\"),\n",
        "    (\"gostei muito da atuação dos atores\", \"positivo\"),\n",
        "    (\"o roteiro é fraco e chato\", \"negativo\"),\n",
        "    (\"não recomendo esse pessimo produto\", \"negativo\"),\n",
        "    (\"uma perda de tempo horrível\", \"negativo\"),\n",
        "    (\"ótimo trabalho, parabéns\", \"positivo\"),\n",
        "    (\"terrível experiência, nunca mais\", \"negativo\"),\n",
        "    (\"excelente serviço, muito eficiente\", \"positivo\"),\n",
        "    (\"que decepção, muito ruim\", \"negativo\"),\n",
        "    (\"aprendizagem de máquina é fascinante\", \"positivo\"),\n",
        "    (\"pln é um campo interessante\", \"positivo\"),\n",
        "    (\"este software travou varias vezes\", \"negativo\"),\n",
        "    (\"a interface é confusa e difícil\", \"negativo\"),\n",
        "    (\"o aplicativo é super útil e rápido\", \"positivo\"),\n",
        "]\n",
        "\n",
        "# Separar os textos e os sentimentos em listas distintas\n",
        "# 'textos' conterá apenas as frases\n",
        "# 'sentimentos' conterá apenas os rótulos (positivo ou negativo)\n",
        "\n",
        "textos = [dado[0] for dado in dados_sentimento]        # Extrai os textos (primeiro elemento da tupla)\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]   # Extrai os rótulos (segundo elemento da tupla)\n",
        "\n",
        "# Exibir o total de frases e alguns exemplos\n",
        "print(f\"Total de frases: {len(textos)}\")               # Mostra a quantidade de exemplos no dataset\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")              # Mostra os 3 primeiros textos como exemplo\n",
        "print(f\"Exemplo de sentimentos: {sentimentos[:3]}\")    # Mostra os 3 primeiros rótulos como exemplo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CO2Aa1C1BgD",
        "outputId": "4176fa09-78c3-4813-fe14-43d59cb7d618"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['este filme é ótimo e divertido', 'eu adorei o livro, muito bom', 'gostei muito da atuação dos atores']\n",
            "Exemplo de sentimentos: ['positivo', 'positivo', 'positivo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapear os sentimentos de texto para valores numéricos\n",
        "# Isso é necessário porque os modelos de aprendizado de máquina trabalham com números, não com strings\n",
        "\n",
        "mapeamento_sentimento = {'negativo': 0, 'positivo': 1}  # Dicionário que converte rótulos de texto para números\n",
        "\n",
        "# Aplicar o mapeamento a todos os rótulos da lista de sentimentos\n",
        "# Cada item da lista 'sentimentos' será convertido com base no dicionário\n",
        "rotulos_numericos = np.array([mapeamento_sentimento[s] for s in sentimentos])\n",
        "\n",
        "# Exibir o resultado da conversão dos rótulos para formato numérico\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byn9DY3r1KYg",
        "outputId": "9c839865-45f7-43d9-8a52-ce0bee386e31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentimentos mapeados para números: [1 1 1 0 0 0 1 0 1 0 1 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenização de Texto\n",
        "# Converte palavras em índices numéricos com base na frequência\n",
        "\n",
        "tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\")  # num_words=None para considerar todas as palavras; oov_token define um token especial para palavras desconhecidas\n",
        "tokenizer.fit_on_texts(textos)  # Cria o vocabulário a partir dos textos fornecidos\n",
        "sequencias_numericas = tokenizer.texts_to_sequences(textos)  # Converte os textos em listas de índices numéricos\n",
        "\n",
        "# Calcular o tamanho total do vocabulário (adiciona +1 para o índice 0 usado em padding)\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Exibir o vocabulário e as sequências numéricas correspondentes\n",
        "print(f\"\\nVocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"\\nSequencias numéricas das frases: {sequencias_numericas}\")\n",
        "print(f\"\\nTamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências\n",
        "# Determinar o comprimento da frase mais longa para definir o comprimento padrão\n",
        "max_len = max(len(s) for s in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_len}\")\n",
        "\n",
        "# Preencher as sequências com zeros no final (padding pós-sequência) para uniformizar o tamanho\n",
        "sequencias_padded = pad_sequences(sequencias_numericas, maxlen=max_len, padding='post')\n",
        "print(f\"Sequencias após padding: \\n{sequencias_padded}\")\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "# 80% para treino e 20% para teste, garantindo que a proporção entre classes seja mantida (stratify)\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "    sequencias_padded, rotulos_numericos, test_size=0.2, random_state=42, stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "# Exibir as dimensões dos conjuntos de treino e teste\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print(f\"\\nShape de X_teste: {X_teste.shape}\")\n",
        "print(f\"\\nShape de y_treino: {y_treino.shape}\")\n",
        "print(f\"\\nShape de X_teste: {y_teste.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Wzm1mm1Twf",
        "outputId": "24f7903c-b6b0-45ae-f64a-e31fb8da0046"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulário (palavra: índice): {'<unk>': 1, 'é': 2, 'e': 3, 'muito': 4, 'o': 5, 'este': 6, 'ótimo': 7, 'de': 8, 'filme': 9, 'divertido': 10, 'eu': 11, 'adorei': 12, 'livro': 13, 'bom': 14, 'gostei': 15, 'da': 16, 'atuação': 17, 'dos': 18, 'atores': 19, 'roteiro': 20, 'fraco': 21, 'chato': 22, 'não': 23, 'recomendo': 24, 'esse': 25, 'pessimo': 26, 'produto': 27, 'uma': 28, 'perda': 29, 'tempo': 30, 'horrível': 31, 'trabalho': 32, 'parabéns': 33, 'terrível': 34, 'experiência': 35, 'nunca': 36, 'mais': 37, 'excelente': 38, 'serviço': 39, 'eficiente': 40, 'que': 41, 'decepção': 42, 'ruim': 43, 'aprendizagem': 44, 'máquina': 45, 'fascinante': 46, 'pln': 47, 'um': 48, 'campo': 49, 'interessante': 50, 'software': 51, 'travou': 52, 'varias': 53, 'vezes': 54, 'a': 55, 'interface': 56, 'confusa': 57, 'difícil': 58, 'aplicativo': 59, 'super': 60, 'útil': 61, 'rápido': 62}\n",
            "\n",
            "Sequencias numéricas das frases: [[6, 9, 2, 7, 3, 10], [11, 12, 5, 13, 4, 14], [15, 4, 16, 17, 18, 19], [5, 20, 2, 21, 3, 22], [23, 24, 25, 26, 27], [28, 29, 8, 30, 31], [7, 32, 33], [34, 35, 36, 37], [38, 39, 4, 40], [41, 42, 4, 43], [44, 8, 45, 2, 46], [47, 2, 48, 49, 50], [6, 51, 52, 53, 54], [55, 56, 2, 57, 3, 58], [5, 59, 2, 60, 61, 3, 62]]\n",
            "\n",
            "Tamanho total do vocabulário: 63\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "Sequencias após padding: \n",
            "[[ 6  9  2  7  3 10  0]\n",
            " [11 12  5 13  4 14  0]\n",
            " [15  4 16 17 18 19  0]\n",
            " [ 5 20  2 21  3 22  0]\n",
            " [23 24 25 26 27  0  0]\n",
            " [28 29  8 30 31  0  0]\n",
            " [ 7 32 33  0  0  0  0]\n",
            " [34 35 36 37  0  0  0]\n",
            " [38 39  4 40  0  0  0]\n",
            " [41 42  4 43  0  0  0]\n",
            " [44  8 45  2 46  0  0]\n",
            " [47  2 48 49 50  0  0]\n",
            " [ 6 51 52 53 54  0  0]\n",
            " [55 56  2 57  3 58  0]\n",
            " [ 5 59  2 60 61  3 62]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "\n",
            "Shape de X_teste: (3, 7)\n",
            "\n",
            "Shape de y_treino: (12,)\n",
            "\n",
            "Shape de X_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 3: Construção do Modelo LSTM"
      ],
      "metadata": {
        "id": "D3F6yxG21YJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura do modelo LSTM\n",
        "\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# Camada de Embedding: Converte os índices numéricos das palavras em vetores densos.\n",
        "# total_palavras_vocab: tamanho do vocabulário\n",
        "# 50: dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequencias (max_len)\n",
        "modelo_lstm.add(Embedding(total_palavras_vocab, 50, input_length=max_len))\n",
        "\n",
        "# Camada LSTM:\n",
        "# 64: número de unidades (neuronios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropou: Um tipo de regularização para evitar overfitting (descarta aleatoriamente neuronios durante o treinamento)\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Camada Densa de Saída:\n",
        "# 1: Um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': Função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "xJlYLtSf1anz",
        "outputId": "830194bf-0fab-44f6-bc4b-0670b616e987"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 4: Treinamento e Avaliação do Modelo"
      ],
      "metadata": {
        "id": "ARJB6y-g1dme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "\n",
        "# Ajusta o modelo aos dados de treino\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino,           # Dados de entrada para treinamento\n",
        "    y_treino,           # Rótulos correspondentes\n",
        "    epochs=50,          # Número de épocas (quantas vezes o modelo verá todos os dados)\n",
        "    batch_size=2,       # Número de amostras processadas antes de atualizar os pesos\n",
        "    validation_split=0.1,  # Percentual dos dados de treino usados para validação (10%)\n",
        "    verbose=1           # Nível de verbosidade: 1 mostra barra de progresso e métricas\n",
        ")\n",
        "\n",
        "# Exibir confirmação após o treinamento\n",
        "print(\"Treinamento concluído!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0opC3zzs1lS7",
        "outputId": "aecdf088-8be0-4203-c1a2-8d9e9da1d50d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.5347 - loss: 0.6940 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7931 - loss: 0.6888 - val_accuracy: 0.5000 - val_loss: 0.6907\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9458 - loss: 0.6829 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.6813 - val_accuracy: 0.5000 - val_loss: 0.6915\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.6729 - val_accuracy: 0.5000 - val_loss: 0.6940\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.6614 - val_accuracy: 0.5000 - val_loss: 0.6973\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.6410 - val_accuracy: 0.5000 - val_loss: 0.7039\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.6023 - val_accuracy: 0.5000 - val_loss: 0.7211\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.5289 - val_accuracy: 0.0000e+00 - val_loss: 0.7651\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.3812 - val_accuracy: 0.0000e+00 - val_loss: 0.8660\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2222 - val_accuracy: 0.0000e+00 - val_loss: 1.1241\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1115 - val_accuracy: 0.0000e+00 - val_loss: 1.6162\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0337 - val_accuracy: 0.0000e+00 - val_loss: 2.2426\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0161 - val_accuracy: 0.0000e+00 - val_loss: 2.8245\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.0000e+00 - val_loss: 3.2806\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.0000e+00 - val_loss: 3.6203\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.0000e+00 - val_loss: 3.8684\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.0000e+00 - val_loss: 4.0532\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.0000e+00 - val_loss: 4.2031\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.0000e+00 - val_loss: 4.3196\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0026 - val_accuracy: 0.0000e+00 - val_loss: 4.4091\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.0000e+00 - val_loss: 4.4829\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.0000e+00 - val_loss: 4.5467\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.0000e+00 - val_loss: 4.6085\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.0000e+00 - val_loss: 4.6622\n",
            "Epoch 26/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.0000e+00 - val_loss: 4.7159\n",
            "Epoch 27/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.0000e+00 - val_loss: 4.7647\n",
            "Epoch 28/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.8092\n",
            "Epoch 29/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.8518\n",
            "Epoch 30/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.0000e+00 - val_loss: 4.8908\n",
            "Epoch 31/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 4.9326\n",
            "Epoch 32/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 4.9747\n",
            "Epoch 33/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.0162\n",
            "Epoch 34/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.0572\n",
            "Epoch 35/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.0000e+00 - val_loss: 5.0991\n",
            "Epoch 36/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.0000e+00 - val_loss: 5.1397\n",
            "Epoch 37/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 9.2572e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.1758\n",
            "Epoch 38/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 9.6964e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2119\n",
            "Epoch 39/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 7.9170e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2474\n",
            "Epoch 40/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.4049e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.2812\n",
            "Epoch 41/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 8.5547e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3163\n",
            "Epoch 42/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 7.7784e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.3497\n",
            "Epoch 43/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.0000e+00 - val_loss: 5.3873\n",
            "Epoch 44/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.8111e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.4203\n",
            "Epoch 45/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 7.0834e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.4519\n",
            "Epoch 46/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 6.2121e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.4829\n",
            "Epoch 47/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 6.8191e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.5121\n",
            "Epoch 48/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 8.0730e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.5447\n",
            "Epoch 49/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 6.2693e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.5765\n",
            "Epoch 50/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 5.4882e-04 - val_accuracy: 0.0000e+00 - val_loss: 5.6080\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "# evaluate retorna a perda (loss) e a acurácia (accuracy) no conjunto de teste\n",
        "# verbose=0 evita a exibição da barra de progresso\n",
        "\n",
        "# Exibe os resultados da avaliação\n",
        "print(f\"\\nAcurácia do modelo no conjunto de teste: {acuracia*100:.2f}%\")  # Acurácia em percentual\n",
        "print(f\"Perda do modelo no conjunto de teste: {perda:.4f}\")               # Perda com 4 casas decimais\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)  # Retorna probabilidades de cada amostra pertencer à classe 1\n",
        "y_pred_classes = (y_pred_prob > 0.5).astype(int)  # Converte probabilidades em rótulos binários (0 ou 1)\n",
        "\n",
        "# Exibir relatório visual com matriz de confusão\n",
        "print(\"\\n --- Relatório de Classificação ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred_classes)  # Calcula a matriz de confusão\n",
        "sns.heatmap(\n",
        "    cm,                      # Matriz a ser exibida\n",
        "    annot=True,              # Mostrar os valores nas células\n",
        "    fmt='d',                 # Formato dos valores inteiros\n",
        "    cmap='Blues',            # Mapa de cores\n",
        "    xticklabels=['negativo', 'positivo'],  # Rótulos do eixo x\n",
        "    yticklabels=['negativo', 'positivo']   # Rótulos do eixo y\n",
        ")\n",
        "plt.xlabel('Previsto')       # Rótulo do eixo x\n",
        "plt.ylabel('Real')           # Rótulo do eixo y\n",
        "plt.title('Matriz de confusão')  # Título do gráfico\n",
        "plt.show()                   # Exibe o gráfico\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "6eVcSWyk1wIk",
        "outputId": "39a77dbe-4a0d-4d73-fdef-2ec212ffafc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Acurácia do modelo no conjunto de teste: 33.33%\n",
            "Perda do modelo no conjunto de teste: 1.2662\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
            "\n",
            " --- Relatório de Classificação ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 5: Testar o Modelo com Novas Frases"
      ],
      "metadata": {
        "id": "62ROryDM1zRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para prever o sentimento de uma nova frase usando o modelo treinado\n",
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimento):\n",
        "  # Converter a frase para uma sequência numérica baseada no vocabulário treinado\n",
        "  sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])\n",
        "\n",
        "  # Verifica se a sequência está vazia (ou seja, nenhuma palavra foi reconhecida)\n",
        "  if not sequencia_numerica:\n",
        "    print(f\"Aviso: A frase '{frase_nova}' contém apenas palavras desconhecidas.\")\n",
        "    return \"Desconhecido\"  # Retorna uma indicação de que a frase não pôde ser processada\n",
        "\n",
        "  # Extraí a primeira sequência (única neste caso)\n",
        "  sequencia_numerica = sequencia_numerica[0]\n",
        "\n",
        "  # Padroniza o comprimento da sequência (como foi feito no treinamento)\n",
        "  sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='post')\n",
        "\n",
        "  # Faz a previsão de sentimento (retorna uma probabilidade entre 0 e 1)\n",
        "  probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0)[0][0]\n",
        "\n",
        "  # Inverte o dicionário de mapeamento para obter os nomes a partir dos números\n",
        "  mapeamento_inverso = {v: k for k, v in mapeamento_sentimento.items()}\n",
        "\n",
        "  # Classifica o sentimento com base na probabilidade (> 0.5 é positivo)\n",
        "  if probabilidade_positiva >= 0.5:\n",
        "    return mapeamento_inverso[1]  # Retorna \"positivo\"\n",
        "  else:\n",
        "    return mapeamento_inverso[0]  # Retorna \"negativo\"\n",
        "\n",
        "# Testa o modelo com frases novas e exibe o resultado\n",
        "\n",
        "print(\"\\n --- Testando o Modelo LSTM com novas Frases ---\")\n",
        "\n",
        "frase_nova_1 = \"gostei muito do filme, excelente!\"\n",
        "sentimento_1 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_1, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\")\n",
        "\n",
        "frase_nova_2 = \"odiei o livro, muito entediante\"\n",
        "sentimento_2 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_2, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_2}' -> Sentimento previsto: '{sentimento_2}'\")  # Corrigido o erro: estava imprimindo sentimento_1\n",
        "\n",
        "frase_nova_3 = \"a aula de pln é ótima!\"\n",
        "sentimento_3 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_3, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_3}' -> Sentimento previsto: '{sentimento_3}'\")\n",
        "\n",
        "frase_nova_4 = \"o atendimento foi péssimo\"\n",
        "sentimento_4 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_4, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_4}' -> Sentimento previsto: '{sentimento_4}'\")\n",
        "\n",
        "frase_nova_5 = \"esse produto não vale a pena, é caro\"\n",
        "sentimento_5 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_5, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_5}' -> Sentimento previsto: '{sentimento_5}'\")\n",
        "\n",
        "frase_nova_6 = \"o filme é legal\"\n",
        "sentimento_6 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_6, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_6}' -> Sentimento previsto: '{sentimento_6}'\")\n",
        "\n",
        "frase_nova_7 = \"isso é horrível, que tristeza\"\n",
        "sentimento_7 = prever_sentimento(modelo_lstm, tokenizer, max_len, frase_nova_7, mapeamento_sentimento)\n",
        "print(f\"Frase: '{frase_nova_7}' -> Sentimento previsto: '{sentimento_7}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iFhtAfR19_V",
        "outputId": "bd977184-e7f7-48c8-c0f0-85376337d7e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " --- Testando o Modelo LSTM com novas Frases ---\n",
            "Frase: 'gostei muito do filme, excelente!' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'odiei o livro, muito entediante' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'a aula de pln é ótima!' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'o atendimento foi péssimo' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'esse produto não vale a pena, é caro' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'o filme é legal' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'isso é horrível, que tristeza' -> Sentimento previsto: 'negativo'\n"
          ]
        }
      ]
    }
  ]
}